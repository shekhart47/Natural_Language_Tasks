{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from pickle import dump\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.utils.vis_utils import plot_model \n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    file = open(filename,'r')\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'republican_clean.txt'\n",
    "data = load_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(file):\n",
    "    file = file.replace('--',' ')\n",
    "    \n",
    "    tokens = data.split()\n",
    "    \n",
    "    re_punct = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    cleaned_text = [re_punct.sub('',w) for w in tokens]\n",
    "    cleaned_text = [word for word in cleaned_text if word.isalpha()]\n",
    "    cleaned_text = [word.lower() for word in cleaned_text]\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_doc = clean_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size :  118214\n",
      "Unique Vocab :  7837\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size : \", len(cleaned_doc))\n",
    "print(\"Unique Vocab : \", len(set(cleaned_doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(length, cleaned_doc):\n",
    "    # we'll keep each sequence of length 50, so we will input 50 words into the model at a time\n",
    "    # and output one word\n",
    "    \n",
    "    # length here is set to 50, so start is at 51, and end is as at len(cleaned_doc)\n",
    "    # so end becomes 118214\n",
    "    \n",
    "    # line will ultimately range from 0 : 50 which is 51 words in total, first 50 of which\n",
    "    # would be input words and the last one (50th word) would be the output word\n",
    "    start = length + 1\n",
    "    end = len(cleaned_doc)\n",
    "    sequences = []\n",
    "    for i in range(start, end):\n",
    "        line = cleaned_doc[i - start : i]\n",
    "        \n",
    "        line = ' '.join(line)\n",
    "        \n",
    "        sequences.append(line)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50\n",
    "sequences = create_sequences(length, cleaned_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_file(sequences, filename):\n",
    "    lines = '\\n'.join(sequences)\n",
    "    file = open(filename,'w')\n",
    "    file.write(lines)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'republic_sequences.txt'\n",
    "sequence_to_file(sequences, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab_size = len(tokenizer.word_index)+1\n",
    "    encoded = np.array(tokenizer.texts_to_sequences(data))\n",
    "    X = np.array(encoded[:,:-1])\n",
    "    y = np.array(encoded[:,-1])\n",
    "    y = np.array(to_categorical(y, num_classes = vocab_size))\n",
    "    return X,y,tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = load_file('republic_sequences.txt')\n",
    "lines = lines.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,tokenizer, vocab_size = generate_data(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (118163, 50)\n",
      "y shape :  (118163, 7838)\n",
      "Vocabulary Size :  7838\n",
      "Sequence length :  50\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape : \",X.shape)\n",
    "print(\"y shape : \",y.shape)\n",
    "print(\"Vocabulary Size : \",vocab_size)\n",
    "print(\"Sequence length : \", X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(vocab_size,sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, output_dim = 50, input_length = sequence_length))\n",
    "    model.add(LSTM(100, activation = 'relu', return_sequences= True))\n",
    "    model.add(LSTM(100, activation = 'relu'))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = make_model(vocab_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "118163/118163 [==============================] - 148s 1ms/step - loss: 6.3859 - acc: 0.0568\n",
      "Epoch 2/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 6.0553 - acc: 0.0699\n",
      "Epoch 3/100\n",
      "118163/118163 [==============================] - 144s 1ms/step - loss: 5.7584 - acc: 0.1038\n",
      "Epoch 4/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 5.4621 - acc: 0.1323\n",
      "Epoch 5/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 5.2583 - acc: 0.1464\n",
      "Epoch 6/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 5.0892 - acc: 0.1592\n",
      "Epoch 7/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 4.9391 - acc: 0.1700\n",
      "Epoch 8/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 4.8008 - acc: 0.1783\n",
      "Epoch 9/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 4.6780 - acc: 0.1859\n",
      "Epoch 10/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 4.5618 - acc: 0.1926\n",
      "Epoch 11/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 4.4517 - acc: 0.1993\n",
      "Epoch 12/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 4.3418 - acc: 0.2050\n",
      "Epoch 13/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 4.2357 - acc: 0.2127\n",
      "Epoch 14/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 4.1277 - acc: 0.2193\n",
      "Epoch 15/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 4.0207 - acc: 0.2262\n",
      "Epoch 16/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.9171 - acc: 0.2329\n",
      "Epoch 17/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 3.8149 - acc: 0.2420\n",
      "Epoch 18/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 3.7156 - acc: 0.2515\n",
      "Epoch 19/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.6204 - acc: 0.2613\n",
      "Epoch 20/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.5309 - acc: 0.2724\n",
      "Epoch 21/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.4431 - acc: 0.2835\n",
      "Epoch 22/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.3599 - acc: 0.2937\n",
      "Epoch 23/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.2840 - acc: 0.3050\n",
      "Epoch 24/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 3.2112 - acc: 0.3155\n",
      "Epoch 25/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.1407 - acc: 0.3243\n",
      "Epoch 26/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.0740 - acc: 0.3352\n",
      "Epoch 27/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 3.0103 - acc: 0.3438\n",
      "Epoch 28/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.9481 - acc: 0.3536\n",
      "Epoch 29/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 2.8900 - acc: 0.3621\n",
      "Epoch 30/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.8371 - acc: 0.3714\n",
      "Epoch 31/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.7844 - acc: 0.3800\n",
      "Epoch 32/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.7335 - acc: 0.3877\n",
      "Epoch 33/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.6817 - acc: 0.3950\n",
      "Epoch 34/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.6371 - acc: 0.4035\n",
      "Epoch 35/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.5947 - acc: 0.4125\n",
      "Epoch 36/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.5516 - acc: 0.4175\n",
      "Epoch 37/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.5129 - acc: 0.4239\n",
      "Epoch 38/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.4681 - acc: 0.4324\n",
      "Epoch 39/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.4270 - acc: 0.4401\n",
      "Epoch 40/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.3948 - acc: 0.4465\n",
      "Epoch 41/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 2.3538 - acc: 0.4528\n",
      "Epoch 42/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 2.3242 - acc: 0.4589\n",
      "Epoch 43/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.2870 - acc: 0.4649\n",
      "Epoch 44/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.2569 - acc: 0.4716\n",
      "Epoch 45/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.2224 - acc: 0.4766\n",
      "Epoch 46/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.1951 - acc: 0.4830\n",
      "Epoch 47/100\n",
      "118163/118163 [==============================] - 145s 1ms/step - loss: 2.1573 - acc: 0.4890\n",
      "Epoch 48/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.1343 - acc: 0.4923\n",
      "Epoch 49/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 2.1025 - acc: 0.4994\n",
      "Epoch 50/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 2.0744 - acc: 0.5051\n",
      "Epoch 51/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.0516 - acc: 0.5095\n",
      "Epoch 52/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.0260 - acc: 0.5146\n",
      "Epoch 53/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 2.0066 - acc: 0.5174\n",
      "Epoch 54/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.9715 - acc: 0.5240\n",
      "Epoch 55/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.9575 - acc: 0.5270\n",
      "Epoch 56/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.9285 - acc: 0.5334\n",
      "Epoch 57/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.9031 - acc: 0.5368\n",
      "Epoch 58/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.8810 - acc: 0.5416\n",
      "Epoch 59/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.8586 - acc: 0.5457\n",
      "Epoch 60/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.8387 - acc: 0.5508\n",
      "Epoch 61/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.8094 - acc: 0.5564\n",
      "Epoch 62/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.7907 - acc: 0.5601\n",
      "Epoch 63/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.7787 - acc: 0.5620\n",
      "Epoch 64/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.7571 - acc: 0.5673\n",
      "Epoch 65/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.7337 - acc: 0.5713\n",
      "Epoch 66/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.7281 - acc: 0.5730\n",
      "Epoch 67/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.7005 - acc: 0.5790\n",
      "Epoch 68/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.6777 - acc: 0.5843\n",
      "Epoch 69/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.6572 - acc: 0.5874\n",
      "Epoch 70/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.6454 - acc: 0.5903\n",
      "Epoch 71/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.6308 - acc: 0.5928\n",
      "Epoch 72/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.6115 - acc: 0.5970\n",
      "Epoch 73/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.5990 - acc: 0.6003\n",
      "Epoch 74/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.5803 - acc: 0.6037\n",
      "Epoch 75/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.5555 - acc: 0.6081\n",
      "Epoch 76/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.5478 - acc: 0.6104\n",
      "Epoch 77/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.5253 - acc: 0.6155\n",
      "Epoch 78/100\n",
      "118163/118163 [==============================] - 148s 1ms/step - loss: 1.5174 - acc: 0.6166\n",
      "Epoch 79/100\n",
      "118163/118163 [==============================] - 148s 1ms/step - loss: 1.4993 - acc: 0.6206\n",
      "Epoch 80/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.4795 - acc: 0.6249\n",
      "Epoch 81/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.4728 - acc: 0.6279\n",
      "Epoch 82/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.4692 - acc: 0.6267\n",
      "Epoch 83/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.4500 - acc: 0.6322\n",
      "Epoch 84/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.4382 - acc: 0.6334\n",
      "Epoch 85/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.4212 - acc: 0.6377\n",
      "Epoch 86/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.4230 - acc: 0.6369\n",
      "Epoch 87/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3929 - acc: 0.6434\n",
      "Epoch 88/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.3789 - acc: 0.6466\n",
      "Epoch 89/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3673 - acc: 0.6501\n",
      "Epoch 90/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3563 - acc: 0.6515\n",
      "Epoch 91/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3415 - acc: 0.6556\n",
      "Epoch 92/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3362 - acc: 0.6553\n",
      "Epoch 93/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.3263 - acc: 0.6576\n",
      "Epoch 94/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3061 - acc: 0.6635\n",
      "Epoch 95/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3010 - acc: 0.6645\n",
      "Epoch 96/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.3025 - acc: 0.6626\n",
      "Epoch 97/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.2903 - acc: 0.6654\n",
      "Epoch 98/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.2737 - acc: 0.6696\n",
      "Epoch 99/100\n",
      "118163/118163 [==============================] - 146s 1ms/step - loss: 1.2600 - acc: 0.6738\n",
      "Epoch 100/100\n",
      "118163/118163 [==============================] - 147s 1ms/step - loss: 1.2558 - acc: 0.6763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f59b50ef438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, batch_size = 128, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('language_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer,open('tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = load_file('republic_sequences.txt')\n",
    "lines = lines.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = lines[0]\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(n_words, tokenizer, model, sequence_length, seed_text):\n",
    "    input_text = seed_text\n",
    "    results = []\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        padded_text = pad_sequences([encoded], maxlen = sequence_length, padding = 'pre')\n",
    "        y_hat = model.predict_classes(padded_text, verbose = 0)\n",
    "        output_text = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_hat:\n",
    "                output_text = word\n",
    "                break\n",
    "        input_text = input_text + ' ' + output_text\n",
    "        results.append(output_text)\n",
    "    return ' '.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i replied now i meant that you should admit the utility and in this way as i thought i should escape from one of them and then there would remain only the possibility but that little attempt is detected and therefore you will please to give a defence of both well\n",
      "but do you say that there was something who cannot succeed according to me socrates and we may reject as want to exceed the voice of the shepherd that justice gives us who are rich and seen in a few now that he said is the other hand the friction\n"
     ]
    }
   ],
   "source": [
    "seed_text = lines[randint(0, len(lines))]\n",
    "n_words = 50\n",
    "result = generate_sequences(n_words, tokenizer, model, sequence_length, seed_text)\n",
    "print(seed_text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
